---
title: "Classifier Model"
author: "Subhankar, Rohan, Sourabh"
date: "May 24, 2017"
output: html_document
---

**Subhankar Pattnaik** *- 71710059*
**Rohan Sarin** *- 71710071*
**Sourabh Singla** *- 71710091*


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Classifier Model.


```{r}
rm(list=ls())

try(require(RTextTools) || install.packages("RTextTools"))
try(require(tm) || install.packages("tm"))

library("RTextTools")
library("tm")

file.cr = read.csv(paste0("Y:\\Knowledge Repository\\ISB\\TABA\\TABA Group Assignment-20170426\\Football google search.csv"))
file.mi = read.csv(paste0("Y:\\Knowledge Repository\\ISB\\TABA\\TABA Group Assignment-20170426\\Science google search.csv"))
file.lin = read.csv(paste0("Y:\\Knowledge Repository\\ISB\\TABA\\TABA Group Assignment-20170426\\Politics google search.csv"))

file.cr = file.cr[!is.na(file.cr$text)|file.cr$text != '',]

file.mi = file.mi[!is.na(file.mi$text)|file.mi$text != '',]

file.lin = file.lin[!is.na(file.lin$text)|file.lin$text != '',]

file.cr$topic = 1
file.mi$topic = 2
file.lin$topic = 3

combined =  rbind(file.cr,file.mi, file.lin)
```

```{r}
dim(combined)
```

```{r}
names(combined)
```

```{r}
set.seed(18)                          # To fix the sample 
samp_id = sample(1:nrow(combined),
                 round(nrow(combined)*.70),     # 70% records will be used for training
                 replace = F)

train.data = combined[samp_id,]                      # 70% of training data set
test.data = combined[-samp_id,]                      # remaining 30% of training data set

dim(train.data) ; dim(test.data)

train.data$text = tolower(train.data$text)  # Convert to lower case

train.data$text = tolower(train.data$text)  # Convert to lower case 

data = rbind(train.data,test.data)
```

```{r}
text = data$text                      
text = removePunctuation(text)              # remove punctuation marks
text = removeNumbers(text)                  # remove numbers
text = stripWhitespace(text)                # remove blank space
cor = Corpus(VectorSource(text))            # Create text corpus
dtm = DocumentTermMatrix(cor, control = list(weighting =             # Craete DTM
                                               function(x)
                                                 weightTfIdf(x, normalize = F)))
                                                 # weightTf(x)))
training_codes = data$topic       # Coded labels                     # remaining 30% of training data set
```

```{r}
dim(dtm)
```

```{r}
container <- create_container(dtm,t(training_codes),trainSize=1:nrow(train.data), testSize=(nrow(train.data)+1):nrow(data),virgin=FALSE)

models <- train_models(container, algorithms=c("MAXENT")) #"MAXENT","SVM","GLMNET","SLDA","TREE","BAGGING","BOOSTING","RF"
results <- classify_models(container, models)

head(results)


out = data.frame(model_topic = results$MAXENTROPY_LABEL,
                 model_prob = results$MAXENTROPY_PROB,
                 actual_text = data$text[((nrow(train.data)+1):nrow(data))],
                 actual_topic = data$topic[((nrow(train.data)+1):nrow(data))]
)
```

```{r}
(z = as.matrix(table(out$model_topic,out$actual_topic)))
```

```{r}
(pct = round(pct = (sum(diag(z))/sum(z))*100))
```

**So basically the above helps in predicting/tagging tokens to a particular subject i.e. to classify. First we get three files with different subjects and merge them to form a big corpora. After that we separate few percentage of documents from input file to train them and the rest to test them. We clean the corressponding data. After that it is feed to models to generate appropriate classifiers through algorithms like MAXENT, SVM, GLMNET etc. Once the model is built, the test data ie kept separate is taken as input to check whether the model built by train the data is efficient or not. From here, in real scenario once we train, we update the model to give more accurate model. Iterating the input into model gives us more accurate results. We could also try to feed other algorithms and those all will also nearly same accuracy level.**
